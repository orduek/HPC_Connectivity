{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a connectivity analysis pipeline for the KPE study. \n",
    "Methods should be easily generalized for others studies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this box if you want MDSL\n",
    "# using MSDL atlas - Can choose different atlas or different nodes (using ICA or something else)\n",
    "# from nilearn import datasets\n",
    "# atlas = datasets.fetch_atlas_msdl()\n",
    "\n",
    "# # Loading atlas image stored in 'maps'\n",
    "# atlas_filename = atlas['maps']\n",
    "# # Loading atlas data stored in 'labels'\n",
    "# #labels_img=yeo['thick_17']\n",
    "# labels =  atlas['labels']\n",
    "# coords = atlas.region_coords# grab center coordinates for atlas labels\n",
    "# #coords = plotting.find_parcellation_cut_coords(labels_img=labels)\n",
    "# #atlas.region_coords\n",
    "\n",
    "# # optional set of different atlas\n",
    "# #atlas_yeo_2011 = \n",
    "# #atlas_yeo = atlas_yeo_2011.thick_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run either this or MSDL - not both boxes\n",
    "# try to use Shen atlas\n",
    "import pandas as pd\n",
    "from nilearn import plotting\n",
    "atlas_filename = '/home/oad4/nilearn_data/shenParcellation/shen_2mm_268_parcellation.nii.gz'\n",
    "atlas_labes = pd.read_csv('/home/oad4/nilearn_data/shenParcellation/shen_268_parcellation_networklabels.csv')\n",
    "coords = plotting.find_parcellation_cut_coords(labels_img=atlas_filename)\n",
    "\n",
    "atlas_labes = np.array(atlas_labes)\n",
    "atlas_labes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "def removeVars (confoundFile):\n",
    "    # this method takes the csv regressors file (from fmriPrep) and chooses a few to confound. You can change those few\n",
    "    import pandas as pd\n",
    "    confound = pd.read_csv(confoundFile,sep=\"\\t\", na_values=\"n/a\")\n",
    "    finalConf = confound[['csf', 'white_matter', 'framewise_displacement',\n",
    "                          'a_comp_cor_00', 'a_comp_cor_01',\t'a_comp_cor_02', 'a_comp_cor_03', 'a_comp_cor_04', \n",
    "                        'a_comp_cor_05', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']] # can add 'global_signal' also\n",
    "     # change NaN of FD to zero\n",
    "    finalConf = np.array(finalConf)\n",
    "    finalConf[0,2] = 0\n",
    "    return finalConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build method for creating time series for subjects\n",
    "def timeSeries(func_files, confound_files):\n",
    "    total_subjects = [] # creating an empty array that will hold all subjects matrix \n",
    "    # This function needs a masker object that will be defined outside the function\n",
    "    for func_file, confound_file in zip(func_files, confound_files):\n",
    "        print(f\"proccessing file {func_file}\") # print file name\n",
    "        confoundClean = removeVars(confound_file)\n",
    "        confoundArray = confoundClean#confoundClean.values\n",
    "        time_series = masker.fit_transform(func_file, confounds=confoundArray)\n",
    "        #time_series = extractor.fit_transform(func_file, confounds=confoundArray)\n",
    "        #masker.fit_transform(func_file, confoundArray)\n",
    "        total_subjects.append(time_series)\n",
    "    return total_subjects\n",
    "\n",
    "# contrasting two timePoints\n",
    "def contFuncs(time_series1, time_series2):\n",
    "    twoMinusOneMat = []\n",
    "    for scanMatrix, scanMatrix2 in zip(time_series1, time_series2):\n",
    "        a = scanMatrix2 - scanMatrix\n",
    "        twoMinusOneMat.append(a)\n",
    "    return np.array(twoMinusOneMat)\n",
    "\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "\n",
    "# create correlation matrix per subject\n",
    "def createCorMat(time_series):\n",
    "    # create correlation matrix for each subject\n",
    "    fullMatrix = []\n",
    "    for time_s in time_series:\n",
    "        correlation_matrix = correlation_measure.fit_transform([time_s])[0]\n",
    "        fullMatrix.append(correlation_matrix)\n",
    "    return fullMatrix\n",
    "\n",
    "# create connecotme graph per subject\n",
    "def connectome_graph (fullMatrix):\n",
    "    # here it is set to threshold 1%\n",
    "    for matrix in fullMatrix:\n",
    "        plotting.plot_connectome(matrix, coords,\n",
    "                             edge_threshold=\"99%\", colorbar=True)\n",
    "        plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you set the specific methods for masking and correlation. Please see Nilearn website for more info.\n",
    "\n",
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "# in this mask we standardize the values, so mean is 0 and between -1 to 1\n",
    "# masker = NiftiMapsMasker(maps_img=atlas_filename, standardize=True, smoothing_fwhm = 6,\n",
    "#                          memory=\"/home/oad4/scratch60/shenPar_nilearn\",high_pass=.01 , low_pass = .1, t_r=1, verbose=5)\n",
    "\n",
    "# use different masker when using Yeo atlas. \n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True,smoothing_fwhm = 6,\n",
    "                        memory=\"/home/oad4/scratch60/shenPar_nilearn\",high_pass=.01 , low_pass = .1, t_r=1, verbose=5)\n",
    "                           \n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='partial correlation') # can choose partial - it might be better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call subjcets\n",
    "# and start the real analysis\n",
    "subList =  ['008','1293','1307','1322','1339','1343','1387','1223']\n",
    "midSubList = ['1253','1263','1351','1364','1369','1390','1403']\n",
    "\n",
    "# these two functions take subject list and session number (in string) and return func file list and confound file list\n",
    "def fileList(subjects, session):\n",
    "    func_files = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-%s/func/sub-%s_ses-%s_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' % (sub,session,sub,session) for sub in subjects]\n",
    "    return func_files\n",
    "\n",
    "def confList(subjects, session):\n",
    "    confound_files = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-%s/func/sub-%s_ses-%s_task-rest_desc-confounds_regressors.tsv' % (sub,session,sub,session) for sub in subjects]\n",
    "    return confound_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call for the functions for each set.\n",
    "# for every time line we want to run time series and then contrast between the times\n",
    "ket1_series = timeSeries(func_files=fileList(subList,'1'), confound_files=confList(subList, '1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ket2_series = timeSeries(func_files=fileList(subList,'2'), confound_files=confList(subList, '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ket3_series = timeSeries(func_files=fileList(subList,'3'), confound_files=confList(subList, '3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subListKet4 =  ['008','1293','1307','1322','1339','1343','1223']\n",
    "ket4_series = timeSeries(func_files=fileList(subListKet4,'4'), confound_files=confList(subListKet4, '4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build correlation matrix for each time point\n",
    "ket1_corr = createCorMat(ket1_series)\n",
    "ket2_corr = createCorMat(ket2_series)\n",
    "ket3_corr = createCorMat(ket3_series)\n",
    "ket4_corr = createCorMat(ket4_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start contrasting\n",
    "ket2_ket1 = contFuncs(ket1_corr, ket2_corr)\n",
    "ket3_ket1 = contFuncs(ket1_corr, ket3_corr)\n",
    "\n",
    "ket3_ket1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_connectome(np.average(ket2_ket1, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine 7days minux first\")\n",
    "\n",
    "plotting.plot_connectome(np.average(ket3_ket1, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine 30days minux first\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let do midazolam\n",
    "mid1_series = timeSeries(func_files=fileList(midSubList,'1'), confound_files=confList(midSubList, '1'))\n",
    "\n",
    "mid2_series = timeSeries(func_files=fileList(midSubList,'2'), confound_files=confList(midSubList, '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mid3_no1253 = midSubList\n",
    "print(mid3_no1253)\n",
    "mid3_no1253.remove('1253')\n",
    "mid3_series = timeSeries(fileList(mid3_no1253,'3'), confList(mid3_no1253,'3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build correlation matrix for each time point\n",
    "mid1_corr = createCorMat(time_series=mid1_series)\n",
    "mid2_corr = createCorMat(mid2_series)\n",
    "mid3_corr = createCorMat(mid3_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start contrasting\n",
    "mid2_mid1 = contFuncs(mid1_corr, mid2_corr)\n",
    "ket3_ket1 = contFuncs(ket1_corr, ket3_corr)\n",
    "\n",
    "mid3_mid1 = contFuncs(mid1_corr[1:], mid3_corr) # removing the first (1253) from the mid1 correlation matrix\n",
    "#np.average(mid3_corr, axis=0) - np.average(mid1_corr, axis = 0) # we do so because the number do not match\n",
    "\n",
    "mid3_mid1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting connectome differences\n",
    "%matplotlib inline\n",
    "\n",
    "plotting.plot_connectome(np.average(ket2_ket1, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine 7days minus first\")\n",
    "\n",
    "plotting.plot_connectome(np.average(ket3_ket1, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine 30days minus first\")\n",
    "\n",
    "plotting.plot_connectome(np.average(mid2_mid1, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Midazolam 7days minus first\")\n",
    "\n",
    "#plotting.plot_connectome(mid3_mid1, coords,\n",
    " #                        edge_threshold=\"99%\", colorbar=True, title = \"midazolam 30days minux first\")\n",
    "\n",
    "plotting.plot_connectome(np.average(ket1_corr,axis=0) -np.average(mid1_corr, axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine - Midazolam 0days\")\n",
    "\n",
    "plotting.plot_connectome(np.average(ket2_corr,axis=0)-np.average(mid2_corr,axis=0), coords,\n",
    "                         edge_threshold=\"99%\", colorbar=True, title = \"Ketamine-Midazolan 7days\", output_file='pretty_brain.png')\n",
    "\n",
    "\n",
    "plotting.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the array to N,N,subnumber for NBS\n",
    "ket1Reshape = np.moveaxis(np.array(ket1_corr), 0,-1)\n",
    "print(ket1Reshape.shape)\n",
    "ket2Reshape = np.moveaxis(np.array(ket2_corr), 0,-1)\n",
    "ket3Reshape = np.moveaxis(np.array(ket3_corr), 0,-1)\n",
    "ket4Reshape = np.moveaxis(np.array(ket4_corr), 0,-1)\n",
    "mid1Reshape = np.moveaxis(np.array(mid1_corr),0,-1)\n",
    "mid2Reshape = np.moveaxis(np.array(mid2_corr),0,-1)\n",
    "mid3Reshape = np.moveaxis(np.array(mid3_corr),0,-1)\n",
    "\n",
    "#print(mid3Reshape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can run NBS\n",
    "# NBS is taken from: https://github.com/aestrivex/bctpy, can be installed using pip (pip install bctpy)\n",
    "from bct import nbs\n",
    "# we compare ket1 and ket3\n",
    "pval, adj, _ = nbs.nbs_bct(ket1Reshape, ket3Reshape, thresh=3, tail='both',k=1000, paired=True, verbose = True)\n",
    "# check mean p vlue\n",
    "#np.mean(checkNBS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at p values and No. of components.\n",
    "print(pval.shape)\n",
    "print (pval)\n",
    "len(pval)\n",
    "print(adj.shape)\n",
    "\n",
    "print(adj[0:10])\n",
    "ad = np.array(adj)\n",
    "print(ad[:,0:10])\n",
    "#bct.adjacency_plot_und(adj, coords, tube=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame from adjacency matrix\n",
    "# here labels has two columns. Need only one\n",
    "import pandas as pd\n",
    "adDat = pd.DataFrame(ad, columns=atlas_labes[:,0],index=atlas_labes[:,0])\n",
    "print(adDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph adjacency matrix\n",
    "import networkx as nx\n",
    "#G = nx.from_numpy_matrix(np.array(ad)) \n",
    "#G = nx.DiGraph(adDat, with_labels=True)\n",
    "G = nx.from_pandas_adjacency(adDat)\n",
    "# drawing the adajency matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(70,50))\n",
    "nx.draw(G, with_labels=True, font_size = 50, width = 1, alpha = 0.7, font_weight = \"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between 30 days in 1st day Ketamine has different in one components. \n",
    "Lets graph it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at 90 days and first\n",
    "pvalk2, adjk2, _ = nbs.nbs_bct(ket1Reshape, ket3Reshape, thresh=3, tail='both',k=300, paired=True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check Midazolam group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalMid, adjMid, _ = nbs.nbs_bct(mid1Reshape[:,:,:6], mid3Reshape, thresh=2.5, tail='both',k=1000, paired=True, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalMid.shape)\n",
    "print (pvalMid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midazolam has no significant change between first day and 7 or 30 days.\n",
    "Lets check differences between ket and mid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalKetMid, adjKetMid, _ = nbs.nbs_bct(ket2Reshape, mid2Reshape, thresh=2.5, tail='both',k=1000, paired=False, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalKetMid.shape)\n",
    "print (pvalKetMid)\n",
    "# no difference between ketamine and midazolam. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of QA we can create histogram plots of correlation matrices\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib histogram\n",
    "#plt.hist(f) #, color = 'blue', edgecolor = 'black',\n",
    "         #bins = int(180/5))\n",
    "\n",
    "#ket1_corr.hist()\n",
    "#sns.distplot(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all ket subjects\n",
    "#color = sns.cubehelix_palette(len(ket1_corr),8)\n",
    "#color = sns.palplot(sns.color_palette(\"RdBu_r\", len(ket1_corr)))\n",
    "sns.set_palette(\"husl\") # set color pallet\n",
    "correlation_vec = ConnectivityMeasure(kind='partial correlation', vectorize=True) # can choose partial - it might be better\n",
    "    \n",
    "# create correlation matrix for each subject\n",
    "fullVec = []\n",
    "for time_s in ket3_series:\n",
    "    cor = correlation_vec.fit_transform([time_s])[0]\n",
    "    print(cor.shape)\n",
    "    plt.hist(cor, alpha = 0.3)\n",
    "    fullVec.append(cor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loop through adj matrix\n",
    "# every time find 1 index it and take actual corelation deltas from Ket3-ket1 matrix (or other of that kind)\n",
    "#ketDeltaA = []\n",
    "#for i in ket3_ket1:\n",
    " #   ketSubjectAdj = []#[np.zeros((39,39))]\n",
    "  #  for n,k in zip(ad,i):\n",
    "        \n",
    "        # now row by row\n",
    "   #     for rAdj, rKet in zip(n,k):\n",
    "    #        a = []\n",
    "            \n",
    "     #       if rAdj!=0:\n",
    "       #         a.insert(len(a),rKet) # inserting another element\n",
    "      #      else:\n",
    "        #        a.insert(len(a),0)\n",
    "         #   ketSubjectAdj.insert(len(ketSubjectAdj),a)\n",
    "    #ketDeltaA.append(np.array(ketSubjectAdj))#.reshape(-1)) # flatten the ,1 matrix to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adjacency matrix - take the real correaltion differences \n",
    "ketDeltaA = []\n",
    "b = ad\n",
    "for ket in (ket3_ket1):\n",
    "    g = np.zeros([268,268])\n",
    "    print(ket[b!=0])\n",
    "    g[b!=0] = ket[b!=0]\n",
    "    ketDeltaA.insert(len(ketDeltaA),g)\n",
    "    \n",
    "np.array(ketDeltaA).shape  \n",
    "#print(f\"First {np.array(ketDeltaA)[0,:,0:10]}\")\n",
    "#print(f\"Sec {np.array(ketDeltaA)[1,:,0:10]}\")\n",
    "    \n",
    "# This vectors (one per subject) will use for regression analysis, after we exclude all zeros from them (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating same vector for midazolam group - just to help us comparing\n",
    "midDeltaA = []\n",
    "b = ad\n",
    "for ket in (mid3_mid1):\n",
    "    g = np.zeros([39,39])\n",
    "    print(ket[b!=0])\n",
    "    g[b!=0] = ket[b!=0]\n",
    "    midDeltaA.insert(len(midDeltaA),g)\n",
    "    \n",
    "np.array(midDeltaA).shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ([1,2,3],[4,5,6], [7,8,9])\n",
    "q = ([1,2,7],[4,5,9])\n",
    "x = ([0,0,1],[0,0,1])\n",
    "x= np.array(x)\n",
    "p = np.array(p)\n",
    "print(p)\n",
    "print(p.reshape(-1))\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDeltaA = ketDeltaA + midDeltaA 3 combine all together\n",
    "np.array(allDeltaA).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will build a simple average matrix (ket3 - ket1) according to the adjacency matrix (39,39) in order to vizualize\n",
    "# results on the nodes themselves\n",
    "ket3_1_average = np.average(np.array(ket3_ket1), axis = 0)\n",
    "\n",
    "# now we replace all 1 in adj to real values and zeros will remain zeros\n",
    "j = adj\n",
    "j[j!=0] = ket3_1_average[j!=0] # replacing j values in real difference instead of 1\n",
    "# now we can plot\n",
    "\n",
    "plotting.plot_connectome(j, coords,\n",
    "                        edge_threshold=\"98%\", colorbar=True)#, output_file=\"Ket3_vsKet1.png\")\n",
    "\n",
    "\n",
    "plotting.plot_matrix(j, labels=atlas_labes, colorbar=True,\n",
    "                     vmax=0.3, vmin=-0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is a small snippet of code that takes the adajecnt's matrix that was created before and present it in diagonal \n",
    "# correlation matrix. \n",
    "#  change it into dataframe to add labels (using atlas labels)\n",
    "import pandas as pd\n",
    "d = pd.DataFrame(data=j,\n",
    "                 columns=atlas_labes, index=atlas_labes)\n",
    "\n",
    "mask = np.zeros_like(d, dtype=np.bool) # masking half\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns_plot = sns.heatmap(d, mask=mask, cmap=cmap, vmax=.1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now - just to make sure I understand which place in the vector (78 because its 39*2) if for which correlation\n",
    "k = ad\n",
    "k[k==1] = ket3_ket1[0][k==1] # replacing j values in real difference instead of 1\n",
    "\n",
    "sub8 = pd.DataFrame(data=k,\n",
    "                 columns=labels, index=labels)\n",
    "\n",
    "mask = np.zeros_like(sub8, dtype=np.bool) # masking half\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(sub8, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ket3_ket1[1,:,0:15])\n",
    "\n",
    "#print(k[:,38])\n",
    "v = np.array(vecKetamin)\n",
    "print(np.array(ketDeltaA).shape)\n",
    "print(f\"KDeltaArray {np.array(ketDeltaA)[0,21:39,0:10]}\")\n",
    "print(f\"KDeltaVector {np.array(ketDeltaA)[0].reshape(-1)[0:39]}\")\n",
    "print(np.array(vecKetamin)[0])\n",
    "\n",
    "#print(f\"First Ket {ket1_corr[0][:,1:10]}\")\n",
    "#print(f\"Third ket {ket3_corr[0][:,1:10]}\")\n",
    "#print(f\"Substract {ket3_ket1[:,1:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to build regression model according to this vector for each subject\n",
    "# change from matrix to vector\n",
    "vecKetamin = [] # vector of all edges per subject\n",
    "for i in ketDeltaA:\n",
    "    a = np.array(i).reshape(-1) # reshaping subject matrix to vector\n",
    "    #print(a[a!=0]) \n",
    "    vecKetamin.append(a[a!=0]) # appending everything that's not zero \n",
    "np.array(vecKetamin).shape # shape of vector (subjects,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector of all\n",
    "vecAll = [] # vector of all edges per subject\n",
    "for i in allDeltaA:\n",
    "    a = np.array(i).reshape(-1) # reshaping subject matrix to vector\n",
    "    #print(a[a!=0]) \n",
    "    vecAll.append(a[a!=0]) # appending everything that's not zero \n",
    "np.array(vecAll).shape # shape of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting correlation vector - using key=abs for absolute values\n",
    "matSort = []\n",
    "vecSort = []\n",
    "for i in vecKetamin:\n",
    "    print(f\"Original {i}\")\n",
    "    a = sorted(i, key=abs)\n",
    "    print(f\"New {a}\")\n",
    "    vecSort.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vecSort[0]))\n",
    "print(len(set(vecSort[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run regression model to try and predict PCL scores with connectivity vector\n",
    "# first load up pcl data\n",
    "kpe_dat = pd.read_excel('/home/oad4/Documents/kpe_analysis/KPEIHR0009_data_all_scored.xlsx', index_col =\"scr_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "# subset the data frame\n",
    "kpe_pcl = kpe_dat[['pcl5_total_visit7','pcl5_total_screen', 'pcl5_total_followup1','pcl5_total_followup2']] \n",
    "# remove NaN's from FU2\n",
    "for i,n in enumerate(kpe_pcl['pcl5_total_followup2']):\n",
    "    if  pd.isna(n) == True:\n",
    "        print('nan')\n",
    "        print(kpe_pcl['pcl5_total_followup2'][i])\n",
    "        kpe_pcl['pcl5_total_followup2'][i]= np.nanmean(kpe_pcl['pcl5_total_followup2'])\n",
    "    \n",
    "    \n",
    "print(kpe_pcl)\n",
    "np.nanmean(kpe_pcl['pcl5_total_followup2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets build regression model\n",
    "# Y = pcl score at FU2\n",
    "# X's = vector of edges from each subject\n",
    "kpe_data_pcl = kpe_pcl.loc[['KPE008','KPE1293','KPE1307','KPE1322','KPE1339','KPE1343','KPE1387','KPE1223']]\n",
    "print(kpe_data_pcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = ['KPE008','KPE1293','KPE1307','KPE1322','KPE1339','KPE1343','KPE1387','KPE1223']\n",
    "\n",
    "kpe_matrix_dat = pd.DataFrame(vecKetamin, index=su)#pd.DataFrame(vecKetamin, index=su)\n",
    "print(kpe_matrix_dat.iloc[:,0:39])\n",
    "# now we have flatten the matrix. \n",
    "# Remember, we now have double of everything, saw basically should split it in half. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = kpe_matrix_dat.iloc[:,0:4] #kpe_matrix_dat[1]\n",
    "y = kpe_data_pcl[\"pcl5_total_followup2\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDF = kpe_data_pcl\n",
    "allDF.merge(kpe_matrix_dat)\n",
    "print(allDF)\n",
    "#fig, ax = plt.subplots(figsize=(12,8))\n",
    "#fig = sm.graphics.plot_partregress(kpe_matrix_dat[:,0], kpe_data_pcl[\"pcl5_total_followup2\"], [kpe_matrix_dat[:,0:5],kpe_data_pcl[\"pcl5_total_followup2\"]] , ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing results as .tex file (so we can convert later to PDF or PNG)\n",
    "beginningtex = \"\"\"\\\\documentclass{report}\n",
    "\\\\usepackage{booktabs}\n",
    "\\\\begin{document}\"\"\"\n",
    "endtex = \"\\end{document}\"\n",
    "\n",
    "f = open('myreg.tex', 'w')\n",
    "f.write(beginningtex)\n",
    "f.write(model.summary().as_latex())\n",
    "f.write(endtex)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing same regression model for all subjects\n",
    "# first data frame\n",
    "all_data_pcl = kpe_pcl.loc[['KPE008','KPE1293','KPE1307','KPE1322','KPE1339','KPE1343','KPE1387','KPE1223','KPE1263','KPE1351','KPE1364','KPE1369','KPE1390','KPE1403']]\n",
    "\n",
    "suAll = ['KPE008','KPE1293','KPE1307','KPE1322','KPE1339','KPE1343','KPE1387','KPE1223','KPE1263','KPE1351','KPE1364','KPE1369','KPE1390','KPE1403']\n",
    "\n",
    "all_matrix_dat = pd.DataFrame(vecAll, index=suAll)\n",
    "\n",
    "X = all_matrix_dat.iloc[:,0:5] #kpe_matrix_dat[1]\n",
    "y = all_data_pcl[\"pcl5_total_followup2\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "# Note the difference in argument order\n",
    "modelAll = sm.OLS(y, X).fit()\n",
    "predictions = modelAll.predict(X) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "print(modelAll.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# X = kpe_matrix_dat #kpe_matrix_dat[1]\n",
    "# y = kpe_data_pcl[\"pcl5_total_followup2\"]\n",
    "\n",
    "\n",
    "X = all_matrix_dat #kpe_matrix_dat[1]\n",
    "y = all_data_pcl[\"pcl5_total_followup2\"]\n",
    "# define the data/predictors as the pre-set feature names  \n",
    "#df = pd.DataFrame(kpe_matrix_dat)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "model = lm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bct\n",
    "fig=bct.adjacency_plot_und(A = ad, coor = coords)\n",
    "from mayavi import mlab \n",
    "mlab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X,y)\n",
    "# This leads to perfect prediction. So lets split data for test-retest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)\n",
    "# prediction is not great"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
