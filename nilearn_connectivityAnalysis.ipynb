{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from nilearn.plotting import plot_prob_atlas\n",
    "from nilearn.plotting import plot_stat_map, show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subList =  ['008', '1253' , '1293' ,'1307','1322','1339','1343','1387'] #'1223'\n",
    "midSubList = ['1253','1263','1351','1364','1369','1390','1403']\n",
    "\n",
    "subject_list = np.concatenate([subList,midSubList])\n",
    "rest_files_1 = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-1/func/sub-%s_ses-1_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' % (sub,sub) for sub in subList]\n",
    "\n",
    "confound_files_1 = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-1/func/sub-%s_ses-1_task-rest_desc-confounds_regressors.tsv' % (sub,sub) for sub in subList]\n",
    "\n",
    "\n",
    "rest_files_2 = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-2/func/sub-%s_ses-2_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' % (sub,sub) for sub in subList]\n",
    "\n",
    "confound_files_2 = ['/home/oad4/scratch60/kpeOutput/fmriprep/sub-%s/ses-2/func/sub-%s_ses-2_task-rest_desc-confounds_regressors.tsv' % (sub,sub) for sub in subList]\n",
    "kpe_label = [1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0]\n",
    "\n",
    "rest_files = np.concatenate([rest_files_1, rest_files_2])\n",
    "confound_files = np.concatenate([confound_files_1, confound_files_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decomposition import CanICA\n",
    "canica = CanICA(n_components=20, smoothing_fwhm=6.,\n",
    "                memory=\"/home/oad4/scratch60/nilearn\", memory_level=2,\n",
    "                threshold=\"auto\", verbose=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canica.fit(rest_files_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = canica.masker_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate(masker.transform(rest_files_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = canica._raw_score(data, per_component=True)\n",
    "\n",
    "plt.plot(scores, '-o')\n",
    "plt.title('Explained variance for 20 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "canica.fit(rest_files_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = canica.masker_\n",
    "data2 = np.concatenate(masker.transform(rest_files_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = canica._raw_score(data2, per_component=True)\n",
    "\n",
    "plt.plot(scores2, '-o')\n",
    "plt.title('Explained variance for 20 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same but with dictionary learning. \n",
    "from nilearn.decomposition import DictLearning\n",
    "dictlearn = DictLearning(n_components=20, smoothing_fwhm=6.,\n",
    "                memory=\"/home/oad4/scratch60/nilearn\", memory_level=2,\n",
    "                verbose=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictlearn.fit(rest_files_1)\n",
    "component_img1 = dictlearn.components_img_\n",
    "\n",
    "masker = dictlearn.masker_\n",
    "data = np.concatenate(masker.transform(rest_files_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dictlearn._raw_score(data, per_component=True)\n",
    "plt.plot(scores, '-o')\n",
    "plt.title('Explained variance for 20 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictlearn.fit(rest_files_2)\n",
    "\n",
    "masker = dictlearn.masker_\n",
    "data2 = np.concatenate(masker.transform(rest_files_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = dictlearn._raw_score(data2, per_component=True)\n",
    "plt.plot(scores2, '-o')\n",
    "plt.title('Explained variance for 20 components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_img2 = dictlearn.components_img_\n",
    "plot_prob_atlas(component_img2, draw_cross=False, linewidths=None,\n",
    "                         cut_coords=[0, 0, 0], title='Dictionary Learning maps 2nd');\n",
    "\n",
    "plot_prob_atlas(component_img1, draw_cross=False, linewidths=None,\n",
    "                         cut_coords=[0, 0, 0], title='Dictionary Learning maps 1st');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import math_img\n",
    "from nilearn import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_img in image.iter_img(component_img1):\n",
    "    plot_stat_map(network_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_img in image.iter_img(component_img2):\n",
    "    plot_stat_map(network_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import concat_imgs, index_img\n",
    "single_mn_image_1 = index_img(component_img1, 5) # extracting the specific component. \n",
    "plot_stat_map(single_mn_image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import concat_imgs, index_img\n",
    "single_mn_image_2 = index_img(component_img2, 7) # extracting the specific component. \n",
    "plot_stat_map(single_mn_image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img = math_img(\"img2 - img1\",\n",
    "                      img1=single_mn_image_1, img2=single_mn_image_2)\n",
    "\n",
    "view = plot_stat_map(result_img)\n",
    "view.open_in_browser() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "view = plotting.plot_stat_map(result_img) #, surf_mesh='fsaverage')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img.shape\n",
    "\n",
    "#for i in image.iter_img(result_img):\n",
    " #   plot_stat_map(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
